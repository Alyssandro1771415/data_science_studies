{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.express as px"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_Sensus = pd.read_csv(\"../Machine Learning e Data Science com Python de A à Z/Bases de dados/census.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_credit = pd.read_csv(\"../Machine Learning e Data Science com Python de A à Z/Bases de dados/credit_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_credit = base_credit.iloc[:, 1:4].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_credit = base_credit.iloc[:, 4].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' Para resolver isso podemos seprar cada coluna em novas categorias, \\npor exemplo, existe uma coluna chamada carro e nela temos 3 categorias\\n, como elas seriam divididas com o labelEncoder em 1, 2 e 3 teriamos o problema\\ncitado, para resolver isso criamos uma coluna para cada categoria, nessa, a\\ncategoria que o objeto se encaixa recebe valor 1 e onde não se encaixa \\nrecebe 0'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" O problema de se utilizar apenas o labelEncoder é que podemos ter \n",
    "muitas categorias e os valores serão diferentes, o problema dessa \n",
    "numeração é que, como os algoritmos de aprendizagem de máquina\n",
    "trabalham com várias operações matemáticas e isso pode fazer com \n",
    "que o algoritmo considere números maiores como sendo mais importantes \n",
    "que atributos de numeração mais baixa, mesmo isso indicando apenas\n",
    "a categoria\"\"\"\n",
    "\n",
    "\"\"\" Para resolver isso podemos seprar cada coluna em novas categorias, \n",
    "por exemplo, existe uma coluna chamada carro e nela temos 3 categorias\n",
    ", como elas seriam divididas com o labelEncoder em 1, 2 e 3 teriamos o problema\n",
    "citado, para resolver isso criamos uma coluna para cada categoria, nessa, a\n",
    "categoria que o objeto se encaixa recebe valor 1 e onde não se encaixa \n",
    "recebe 0\"\"\"\n",
    "\n",
    "\n",
    "# Carro\n",
    "# Gol Pálio Uno\n",
    "# 1   2     3\n",
    "\n",
    "#Gol 1 0 0\n",
    "#Pálio 0 1 0\n",
    "#Uno 0 0 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(np.unique(base_Sensus['workclass'])) # Isso implica que teremos 9 novas colunas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "onehotencoder_Sencus = ColumnTransformer(transformers=[('OneHot', OneHotEncoder(),[1, 3, 5, 6, 7, 8, 9, 13])], remainder='passthrough')\n",
    "# remainder='passthrough' serve para não apagar ou outros atributos que não indicamos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_census = base_Sensus.iloc[:, 0:14].values\n",
    "Y_census = base_Sensus.iloc[:, 14].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_census = onehotencoder_Sencus.fit_transform(X_census).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "       0.0000e+00, 0.0000e+00, 1.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "       0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "       0.0000e+00, 0.0000e+00, 0.0000e+00, 1.0000e+00, 0.0000e+00,\n",
       "       0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "       0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 1.0000e+00,\n",
       "       0.0000e+00, 0.0000e+00, 0.0000e+00, 1.0000e+00, 0.0000e+00,\n",
       "       0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "       0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "       0.0000e+00, 0.0000e+00, 0.0000e+00, 1.0000e+00, 0.0000e+00,\n",
       "       0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "       0.0000e+00, 0.0000e+00, 1.0000e+00, 0.0000e+00, 1.0000e+00,\n",
       "       0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "       0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "       0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "       0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "       0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "       0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "       0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "       0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 1.0000e+00,\n",
       "       0.0000e+00, 0.0000e+00, 3.9000e+01, 7.7516e+04, 1.3000e+01,\n",
       "       2.1740e+03, 0.0000e+00, 4.0000e+01])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_census[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32561, 108)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_census.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Escalonamento do Atributos\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler_census = StandardScaler()\n",
    "X_census = scaler_census.fit_transform(X_census)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.2444502 , -0.17429511, -0.26209736, -0.01466381, -1.5167923 ,\n",
       "       -0.18838933, -0.29093568,  4.90769968, -0.02073999, -0.17175325,\n",
       "       -0.19348662, -0.11609195, -0.07201601, -0.10164955, -0.1422718 ,\n",
       "       -0.12664495, -0.18406376, -0.21053433,  2.25399324, -0.11334387,\n",
       "       -0.68994199, -0.23637391, -0.03960742, -0.13419553, -0.53714425,\n",
       "       -0.39750806, -0.02658695, -0.92284068, -0.11403678,  1.43105786,\n",
       "       -0.1802846 , -0.17735813, -0.24494366,  2.76348874, -0.01662771,\n",
       "       -0.37949517, -0.37774555, -0.17745022, -0.20957797, -0.25595432,\n",
       "       -0.33554133, -0.06780164, -0.38166338, -0.14260848, -0.35531609,\n",
       "       -0.17127887, -0.22710355, -0.82533335,  1.70899099, -0.17624972,\n",
       "       -0.42934582, -0.34403232, -0.22492681, -0.09820087, -0.18155194,\n",
       "       -0.32576824, -0.09161163,  0.4130197 , -0.70307135,  0.70307135,\n",
       "       -0.13502327, -0.02416321, -0.06107342, -0.0480488 , -0.04260602,\n",
       "       -0.05409379, -0.04641598, -0.02933708, -0.05714946, -0.05264698,\n",
       "       -0.02985682, -0.06500204, -0.02985682, -0.04437806, -0.03678503,\n",
       "       -0.00554189, -0.01998525, -0.02479131, -0.01998525, -0.05550333,\n",
       "       -0.03636406, -0.02715919, -0.04740236, -0.04993839, -0.04367781,\n",
       "       -0.02351838, -0.1419344 , -0.03233087, -0.02073999, -0.03087016,\n",
       "       -0.07821827, -0.04296623, -0.03372864, -0.05927412, -0.01920091,\n",
       "       -0.0496284 , -0.03960742, -0.02351838, -0.02416321,  0.34095391,\n",
       "       -0.04540836, -0.02217266,  0.03067056, -1.06361075,  1.13473876,\n",
       "        0.1484529 , -0.21665953, -0.03542945])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_census[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bases de Treinamento e teste"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Credito"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_credit_treinamento, X_credit_teste, Y_credit_treinamento, Y_credit_teste = train_test_split(X_credit, Y_credit, test_size=0.25, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1500, 3)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_credit_treinamento.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1500,)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_credit_treinamento.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((500, 3), (500,))"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_credit_teste.shape, Y_credit_teste.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Census\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_census_treinamento,  X_census_teste, Y_census_treinamento, Y_census_teste = train_test_split(X_census, Y_census, test_size=0.15, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((27676, 108), (27676,))"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_census_treinamento.shape, Y_census_treinamento.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((4885, 108), (4885,))"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_census_teste.shape, Y_census_teste.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Salvar as Bases de Dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"credit.pkl\", mode='wb') as f:\n",
    "    pickle.dump([X_credit_treinamento, Y_credit_treinamento, X_credit_teste, Y_credit_teste], f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"census.pkl\", mode='wb') as f:\n",
    "    pickle.dump([X_census_treinamento, Y_census_treinamento, X_census_teste, Y_credit_teste], f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
